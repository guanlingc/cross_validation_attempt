{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c59d381-ed9a-4674-93b6-efc4cfce4ffc",
   "metadata": {},
   "source": [
    "## Importing Modules and Data\n",
    "##### * This section contains code to import modules used in this research notebook\n",
    "##### * It also contains the code to extract the data from the file provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0254f6b8-e0a1-451d-be4e-067b449f4514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, classification_report, confusion_matrix, make_scorer\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "pd.options.display.max_columns = 50 \n",
    "\n",
    "pd.options.display.max_rows = 20\n",
    "\n",
    "print(\"Imported the modules.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a31e08-cfb6-4870-b0d8-9639b0cf9260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to initiate connection to database using sqlite3.\n",
    "# Defining function to pass SQL syntax and convert the obtained data as a pandas Dataframe.\n",
    "\n",
    "def initiate_local_connection():\n",
    "    \"\"\"This function takes in a defined variable called file_name and generates a connection object to it \n",
    "    Parameter:\n",
    "    file_name (object): name of the dataset file  e.g. \"data/survive.db\" or \"calls.csv\"\n",
    "    \n",
    "    Returns:\n",
    "    Connection object \n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(file_name)\n",
    "        print('[+] Local Connection Successful')\n",
    "    except Exception as e: \n",
    "        \n",
    "        print(f'[+] Local Connection Failed: {e}')\n",
    "        conn = None\n",
    "\n",
    "    return conn\n",
    "\n",
    "def get_records(sql_query):\n",
    "    \"\"\" Takes in a SQL syntax to obtain dataset and transform into a pandas dataframe.\n",
    "    Parameter:\n",
    "    sql_query (str): SQL syntax to obtain dataset from connection established in \"initiate_local_connection()\"\n",
    "\n",
    "    Returns:\n",
    "    Pandas dataframe of the information extracted\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # create a cursor object and execute the given SQL syntax \n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(sql_query)\n",
    "\n",
    "        # Fetch all the records from SQL query output\n",
    "        results = cursor.fetchall()\n",
    "        \n",
    "        # Convert results into pandas dataframe\n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        print(f'Successfully retrieved records')\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error encountered: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336f82c0-c074-4dcf-9249-8108aa0e1a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"data/survive.db\"\n",
    "\n",
    "conn = initiate_local_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e981a25b-0328-4a7a-b18a-8f4cd0161193",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = f'SELECT * FROM survive'\n",
    "\n",
    "df = get_records(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785e82a0-304e-4d46-a5c7-4d525a914fee",
   "metadata": {},
   "source": [
    "## Problem statement\n",
    "### * Objective is to predict survival of coronary artery disease using dataset provided.\n",
    "### * The proposed solution should help doctors formulate pre-emptive medical treatments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682b249f-dacf-4e6b-9040-d55969f33b7c",
   "metadata": {},
   "source": [
    "## Preprocessing of data\n",
    "##### * This section will contain the data cleaning performed on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb41ca-fb75-4dc7-8fb0-36ef0115b38f",
   "metadata": {},
   "source": [
    "* First look of the data provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df04409d-9e47-4566-88f3-0c16843ea436",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e4c9e5-54ae-4539-a340-d09f87086003",
   "metadata": {},
   "source": [
    "* Looking at the shape of the dataset that we are provided - 15000 rows, 16 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47af6d2-be70-48dd-b783-0461f88bac8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64371291-62af-42c6-9799-eddd084c799d",
   "metadata": {},
   "source": [
    "* Labelling of the columns based of assessment write up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d307431-1ffd-45be-8072-77a6428d2067",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns = {\n",
    "                    0: 'ID',\n",
    "                    1: 'Survive',\n",
    "                    2: 'Gender',\n",
    "                    3: 'Smoke',\n",
    "                    4: 'Diabetes',\n",
    "                    5: 'Age',\n",
    "                    6: 'Ejection_Fraction',\n",
    "                    7: 'Sodium',\n",
    "                    8: 'Creatinine',\n",
    "                    9: 'Platelets',\n",
    "                    10: 'Creatine_Phosphokinase',\n",
    "                    11: 'Blood_Pressure',\n",
    "                    12: 'Hemoglobin',\n",
    "                    13: 'Height',\n",
    "                    14: 'Weight',\n",
    "                    15: 'Favourite_Color',\n",
    "                    })\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c483bcb-e456-4482-bf6c-b22798197e41",
   "metadata": {},
   "source": [
    "#### * The target variable \"Survive\" is found within the dataset.\n",
    "#### * This can be considered Supervised Learning problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec1831d-645d-4c0e-8f82-38c78cf0b81c",
   "metadata": {},
   "source": [
    "#### Checking and removing of duplicates found within \"ID\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42bf5ba-cbfc-485b-b7ef-9f15130d56e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_id = df.duplicated(subset=['ID']).sum()\n",
    "print(f'There are {duplicated_id} duplicated ID')\n",
    "df.drop_duplicates(subset='ID', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41dc55d-6046-460e-a399-db553f3ed6a4",
   "metadata": {},
   "source": [
    "#### Dropping \"favourite_color\" as it is a irrelevant column for the problem statement.\n",
    "* 14042 rows remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cf3cd1-8828-4253-a3ed-c03029e278c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['ID','Favourite_Color'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3750f07a-6556-4028-b17a-936da5ab22a9",
   "metadata": {},
   "source": [
    "#### Checking the values for each column to see if there are any errors.\n",
    "---\n",
    "* Error detected, Values that can be corrected according to table below\n",
    "\n",
    "|Column Name|Error Type|Correction to perform|Remarks for potential feature engineering|\n",
    "|-|-|-|-|\n",
    "|Survive|Contains '0' and '1'|To be replaced with 'No' and 'Yes' respectively according to write up|Possible binary encoding to 0 and 1|\n",
    "|Smoke|Contains 2 versions of No and Yes|To replace with 'No' and 'Yes' accordingly|Possible binary encoding to 0 and 1|\n",
    "|Diabetes|NA|NA|Possible ordinal encoding for Normal>Pre-diabetes>Diabetes based on severity|\n",
    "|Age|Contains negative values are present within the dataset|Assuming the negative values are a result of entry error, use the absolute value is the correct age|Scaling to be performed as the number range within dataset is varied|\n",
    "|Ejection_Fraction|Contains 'L' and 'N'|To be replaced with 'Low' and 'Normal' assuming that was the intended value|Possible ordinal encoding for Low > Normal > High based on strength |\n",
    "|Sodium|NA|NA|Scaling to be performed as the number range within dataset is varied|\n",
    "|Creatinine|Nan present|To check amount of missing values|Scaling to be performed as the number range within dataset is varied|\n",
    "|Platelets|NA|NA|Scaling to be performed as the number range within dataset is varied|\n",
    "|Creatine_Phosphokinase|NA|NA|Scaling to be performed as the number range within dataset is varied|\n",
    "|Blood_Pressure|NA|NA|Scaling to be performed as the number range within dataset is varied|\n",
    "|Hemoglobin|NA|NA|Scaling to be performed as the number range within dataset is varied|\n",
    "|Height|NA|NA|Possible to create new feature BMI with Weight| \n",
    "|Weight|NA|NA|Possible to create new feature BMI with Height|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d52bc8-4927-4bc7-a9fc-513ae93972b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cols in df.columns:\n",
    "    print (f'The unique values in this column {cols} are:')\n",
    "    print (df[cols].unique())\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb51cca-b2f5-471e-99b3-c4d7c0937c04",
   "metadata": {},
   "source": [
    "#### Replacing the unique values with appropriate ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294a649f-5ca6-4870-b7d8-7838464412f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_value(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"This function takes reads a pandas dataframe and the following\n",
    "    'Survive': Changes all values to 0 and 1 (int)\n",
    "    'Smoke': Changes all values to No and Yes\n",
    "    'Ejection_Fraction': Changes all values to Low, Normal and High \n",
    "    'Age': Negative values are assumed to be entry errors \n",
    "    Parameter:\n",
    "    df: pandas dataframe\n",
    "  \n",
    "    Returns:\n",
    "    df: pandas dataframe with all values replaced\n",
    "    \"\"\"\n",
    "    df['Survive'].replace(['No','Yes'],['0', '1'], inplace=True)\n",
    "    df['Smoke'].replace(['NO','YES'],['No','Yes'], inplace=True)\n",
    "    df['Ejection_Fraction'].replace(['L','N'],['Low','Normal'], inplace=True)\n",
    "    df['Age'] = df['Age'].abs()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c45dfd9-e2c3-4e42-87a0-a2190be6f48a",
   "metadata": {},
   "source": [
    "Checking the results of the transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e926ca6-f27c-411a-b677-871d54afc378",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = replace_value(df)\n",
    "for col in ['Survive', 'Smoke', 'Ejection_Fraction','Age']:\n",
    "    print(df[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b625a1d8-e80a-478f-a14f-8349331e3f3b",
   "metadata": {},
   "source": [
    "#### Counting the amount of missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ab7774-ecf7-4b23-a773-22e5e2a2f4c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e61b62f-ae06-4db2-af06-1675356cbf66",
   "metadata": {},
   "source": [
    "* Calculating the percentage of missing data\n",
    "---\n",
    "* Missing Values contributes to 3.32% of all observations\n",
    "* Due to low missing values, they will be removed from the dataset.\n",
    "* 13576 rows remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4978512b-660b-405c-949e-37e8197cfad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_missing_value_percentage(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" This function takes in a pandas dataframe (df) and returns the percentage of missing values\n",
    "    Parameter:\n",
    "    df: pandas dataframe\n",
    "\n",
    "    Returns:\n",
    "    percentage of missing value\n",
    "    \"\"\"\n",
    "    # Counting missing values in each column\n",
    "    missing_value_counts =  df.isnull().sum()\n",
    "\n",
    "    # Counting the number of observations\n",
    "    total_amount_of_rows = len(df)\n",
    "\n",
    "    # Getting the percentage of that \n",
    "    missing_value_percentage = (missing_value_counts/total_amount_of_rows) * 100\n",
    "\n",
    "    missing_value_df = pd.DataFrame({'column_name': missing_value_percentage.index, \n",
    "                                   'missing_value_percentage': missing_value_percentage.values})\n",
    "    return missing_value_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f26be60-5696-4c45-b5e4-5f033494648c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_value_percentage_df = calculate_missing_value_percentage(df)\n",
    "print(missing_value_percentage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33da22bd-33dd-49b1-b2a8-e96106ac251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Creatinine'])\n",
    "df.isnull().sum()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941aac72-3a70-43f3-9015-c9d8e3320a83",
   "metadata": {},
   "source": [
    "#### Creating a feature called BMI.\n",
    "* Body mass index (BMI) is an estimate using height and weight. Commonly used as a body fat estimate. Healthy range is typically 19 to 24\n",
    "* The formula used to calculate this:\n",
    "BMI = weight (kg) / height_squared (m)\n",
    "\n",
    "* e.g. row 1 = 93kg / (1.8*1.8) = 28.7\n",
    "* Height and Weight row dropped as they are not required anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d5b6a0-2c2a-404b-a2fa-438c0dc20196",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BMI'] = round(df['Weight']/((df['Height']/100)**2),1)\n",
    "df = df.drop(columns=['Height','Weight'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45a95aa-011d-4e2e-8cdc-cd2662965fb4",
   "metadata": {},
   "source": [
    "#### Taking a look at the data types assigned to each columns\n",
    "----\n",
    "Data type is correct for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e7e4b6-a254-46de-8152-8cfdbe43cd93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3229ae-abeb-492e-b29d-7ba23239626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e759a8-55dd-43bb-bd66-a17a41abedc3",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "### Univariate Analysis\n",
    "* Here we split the data into continuous or categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be433fd-6812-41e8-9001-86424a0a0a37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cat_df = df[['Survive','Gender','Smoke','Diabetes','Ejection_Fraction']]\n",
    "cont_df = df[['Age','Sodium','Creatinine','Platelets','Creatine_Phosphokinase','Blood_Pressure','Hemoglobin','BMI','Survive']]\n",
    "# added Target variable into continous df for visualizations later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787de275-b77f-44a5-a48e-0b9696cec9eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking the df output in correct\n",
    "cat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc200c2-5c58-462f-9ccc-c7a54bbca73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the df output in correct\n",
    "cont_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efaf3e8-cf15-46ff-b099-fe16961dcd4d",
   "metadata": {},
   "source": [
    "* For categoricial data, checking the frequency to check the distribution\n",
    "----\n",
    "Findings\n",
    "|Feature|Findings|\n",
    "|-|-|\n",
    "|Survive|32% positive class (survivor), 68% negative class (non-survivor), imbalanced class|\n",
    "|Gender|65% males, 35% females|\n",
    "|Smoke|67% Non-smoker, 33% smokers|\n",
    "|Diabetes|59% Normal, 21% Pre-diabetes, 20% diabetes|\n",
    "|Ejection_Fraction|88.2% weak heart, 11.4% normal, 0.4% strong| "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d767b0-7a49-4ddf-89ee-2f9c8d222d96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for columns in cat_df.columns:\n",
    "    fig = px.histogram(cat_df, x=columns, title=f'Histogram of {columns}', histnorm='percent')\n",
    "    fig.show()                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8cc665-f5ba-4a70-b3c5-c30d0196841b",
   "metadata": {},
   "source": [
    "* For continuous data, checking the frequency to check the distribution\n",
    "---\n",
    "Findings\n",
    "|Feature|Findings|\n",
    "|-|-|\n",
    "|Age|Binning of observation shows a normal distibution|\n",
    "|Sodium|Seems like a normal distibution curve, some outliers detected|\n",
    "|Creatinine|Counts are skewed to the lower range, transformation might be feasible, outliers detected|\n",
    "|Platelets|Seems like a normal distibution curve with some outliers on the high side |\n",
    "|Creatine_Phosphokinase|Counts are skewed to the lower range, transformation might be feasible, some outliers detected|\n",
    "|Blood_Pressure|Distribution seems even|\n",
    "|Hemoglobin|Distribution seems even|\n",
    "|BMI|Seems like a normal distibution curve, binning of observation can be considered|\n",
    "\n",
    "From `.describe()`\n",
    "* Age, Sodium, Creatinie and Platelets have means that are close to 50% indicating even distribution\n",
    "* Creatine_Phosphokinase has a mean close to 75%, suggesting possible outliers and heavily skewing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8d6cad-57a1-4c9f-88f7-468a90579158",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for columns in cont_df.columns:\n",
    "    fig = px.histogram(cont_df, x=columns, \n",
    "                       marginal='box',\n",
    "                       title=f'Histogram of {columns}')\n",
    "    fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b851ae3c-9c54-4e3d-861b-c62253e18479",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = px.histogram(cont_df, x='Age', \n",
    "                   nbins= 12,\n",
    "                   title=f'Histogram of {columns}')\n",
    "fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2ec924-68d8-4a7c-9a0e-1039edcf6238",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = px.histogram(cont_df, x='BMI', \n",
    "                   nbins= 6,\n",
    "                   title=f'Histogram of {columns}')\n",
    "fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e35b4-05dd-4153-9465-b71b44de8351",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cont_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d8301a-116e-469e-adac-beaa1c20982e",
   "metadata": {},
   "source": [
    "## Bivariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab611503-c3ee-43e6-87e4-bc067486c481",
   "metadata": {},
   "source": [
    "* Looking at the proportion of target variable within each categorical feature\n",
    "---\n",
    "Findings when compared with each other\n",
    "|Feature|Findings|\n",
    "|-|-|\n",
    "|Gender|Both genders have approximately half of the number of survivors than non-survivors|\n",
    "|Smoke|Both smokers and non-smokers have approximately half of the number of survivors than non-survivors, only ~5% of smokers are female, non-smokers have equal male vs female|\n",
    "|Diabetes|All diabetes categories have approximately half of the number of survivors than non-survivors, equal occurrences in pre-diabetes and diabetes|\n",
    "|Ejection_Fraction|There were no surviors with High ejection fraction, for Normal category, there are 4x of non-survivors as survivors and for Low category, there are approximately half of the number of survivors than non-survivors and the portion of female increased with higher ejection_fraction|\n",
    "\n",
    "Summary: \n",
    "* Given that this dataset is mild imbalanced (32%) with minority class (positive/survivors), similar ratios are found in Gender,Smoke and Diabetes features.\n",
    "* Ejection_Fraction has a trend on higher mortality (lower chance of survival) with increasing ejection_fractions. \n",
    "\n",
    "Questions derived from observation:\n",
    "* Majority of smokers are males, does it have an influence to survival? No, the proportion is similar to the imbalance of the whole dataset\n",
    "    - An interesting finding, when observations were separated based on Gender and Smoke, only Female smokers had a different ratio, 3x more survivors.\n",
    "* Are High ejection_fraction observations are they all non-diabetic? Yes all of them are non-diabetic and all did not survive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6c5f8f-76bd-4e78-9bf6-656224355f59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for columns in cat_df.columns:\n",
    "    fig = px.histogram(cat_df, x=columns, \n",
    "                       color='Survive',\n",
    "                       title=f'Histogram of {columns}', \n",
    "                        )\n",
    "    fig.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc0db9a-f3f8-4e8d-9f2d-428c668666e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for columns in cat_df.columns:\n",
    "    fig = px.histogram(cat_df, x=columns, \n",
    "                       color='Gender',\n",
    "                       # color_discrete_map={0:'red', 1:'blue'},\n",
    "                       title=f'Histogram of {columns}', \n",
    "                        )\n",
    "    fig.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef795b0-e869-42e2-8117-4cad1e138786",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for columns in cat_df.columns:\n",
    "    fig = px.histogram(cat_df, x=columns, \n",
    "                       color='Smoke',\n",
    "                       # color_discrete_map={0:'red', 1:'blue'},\n",
    "                       title=f'Histogram of {columns}', \n",
    "                        )\n",
    "    fig.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cd517c-ec72-4b6d-942c-d999e7d8f390",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for columns in cat_df.columns:\n",
    "    fig = px.histogram(cat_df, x=columns, \n",
    "                       color='Diabetes',\n",
    "                       # color_discrete_map={0:'red', 1:'blue'},\n",
    "                       title=f'Histogram of {columns}', \n",
    "                        )\n",
    "    fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd10fbe-f9b6-4ffa-ad35-ad3621aa7fda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for columns in cat_df.columns:\n",
    "    fig = px.histogram(cat_df, x=columns, \n",
    "                       color='Ejection_Fraction',\n",
    "                       # color_discrete_map={0:'red', 1:'blue'},\n",
    "                       title=f'Histogram of {columns}', \n",
    "                        )\n",
    "    fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dba5d18-0785-4c7a-8c98-b3b9d4bb922f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gender_smoke_df = df.groupby(['Gender', 'Smoke','Survive'])\n",
    "gender_smoke_df.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38a1c02-d7e5-438e-b093-bd999f9afdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_ef_df = df.groupby(['Diabetes', 'Ejection_Fraction','Survive'])\n",
    "diabetes_ef_df.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040665ac-304a-49ef-a7f7-24bb61182792",
   "metadata": {},
   "source": [
    "* Comparing target variable with the continous features\n",
    "---\n",
    "* Findings when compared with each other\n",
    "|Feature|Findings|\n",
    "|-|-|\n",
    "|Age|NA|\n",
    "|Sodium|NA|\n",
    "|Creatinine|Positive class occupies the higher ranges|\n",
    "|Platelets|NA|\n",
    "|Creatine_Phophokinase|NA|\n",
    "|Blood_Pressure|NA|\n",
    "|Hemoglobin|NA|\n",
    "|BMI|Positive Class occupies the higher value range|\n",
    "\n",
    "Summary: \n",
    "* Creatinine and BMI are the only features that seems to have a difference between the positive and negative classes of the target\n",
    "* Ejection_Fraction has a trend on higher mortality (lower chance of survival) with increasing ejection_fractions.\n",
    "* Not much insights could be drawn when the features were compared to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b771b9a9-e78e-491f-af52-41c2a0edc92f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for columns in cont_df.columns:\n",
    "    fig = px.histogram(cont_df, x=columns,\n",
    "                       color='Survive',\n",
    "                       marginal='box',\n",
    "                       title=f'Histogram of {columns}')\n",
    "    fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef63a99-d063-4265-bf1e-3a9eb5e1257d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for columns in cont_df.columns:\n",
    "    fig = px.scatter(cont_df, x=columns,\n",
    "                       color='Age',\n",
    "                       title=f'Histogram of {columns}')\n",
    "    fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a545584a-9e98-4097-98bc-de720505abfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for columns in cont_df.columns:\n",
    "    fig = px.scatter(cont_df, x=columns,\n",
    "                       color='Sodium',\n",
    "                       title=f'Histogram of {columns}')\n",
    "    fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9d0c18-f9b1-423c-82ae-79dfed9a0dc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for columns in cont_df.columns:\n",
    "    fig = px.scatter(cont_df, x=columns,\n",
    "                       color='Creatinine',\n",
    "                       title=f'Histogram of {columns}')\n",
    "    fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889ad082-b490-4dba-bb47-5e0c4de659ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for columns in cont_df.columns:\n",
    "    fig = px.scatter(cont_df, x=columns,\n",
    "                       color='Platelets',\n",
    "                       title=f'Histogram of {columns}')\n",
    "    fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72af595d-6f5b-44cd-8aab-6911be7650be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for columns in cont_df.columns:\n",
    "    fig = px.scatter(cont_df, x=columns,\n",
    "                       color='Creatine_Phosphokinase',\n",
    "                       title=f'Histogram of {columns}')\n",
    "    fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb52d87-cd25-4c99-af3e-96bf211d04ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for columns in cont_df.columns:\n",
    "    fig = px.scatter(cont_df, x=columns,\n",
    "                       color='Blood_Pressure',\n",
    "                       title=f'Histogram of {columns}')\n",
    "    fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830f2b37-d6b2-4f4d-880c-898f4872e3cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for columns in cont_df.columns:\n",
    "    fig = px.scatter(cont_df, x=columns,\n",
    "                       color='Hemoglobin',\n",
    "                       title=f'Histogram of {columns}')\n",
    "    fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b28061-72a4-4fb4-96cc-0fc9bf9f5fcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for columns in cont_df.columns:\n",
    "    fig = px.scatter(cont_df, x=columns,\n",
    "                       color='BMI',\n",
    "                       title=f'Histogram of {columns}')\n",
    "    fig.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a419ea9c-d4ae-4b3a-8155-b776f8d0a0de",
   "metadata": {},
   "source": [
    "### Log Transformation for skewed columns\n",
    "* 4 columns were observed to have some skewness to it\n",
    "* Based on variance, Platelets, Blood_Pressure, Creatinine and Creatine_Phosphokinase have very high variance.\n",
    "* Log normalization can help to transform the data\n",
    "* Variance for Platelets and Blood_Pressure is reduced.\n",
    "* Skewness of Creatinine and Creatine_Phosphokinase is reduced.\n",
    "* Outliers are still present in log_Platelets and log_Creatinine\n",
    "* All 4 features will assume the log normalization\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d20b29a-4167-40ec-9b8c-708289dc0ff6",
   "metadata": {},
   "source": [
    "Checking variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408070c3-a5e3-416b-b41e-f91e4cb72f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8f9be5-8367-4b0a-a401-a263ec112b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_norm(df: pd.DataFrame, target_column:str) -> pd.DataFrame:\n",
    "    \"\"\"This function takes reads a pandas dataframe creates a log normalization in a new column with the log suffix (log_target_column)\n",
    "    Parameter:\n",
    "    df: pandas dataframe\n",
    "    column_name (str): column to be log normalized\n",
    "    \"\"\"\n",
    "    df[('log_'+ target_column)] = np.log(df[target_column])\n",
    "    return df\n",
    "\n",
    "def log_norm_all(df: pd.DataFrame):\n",
    "    df = log_norm(df, 'Platelets')\n",
    "    df = log_norm(df, 'Creatine_Phosphokinase')\n",
    "    df = log_norm(df, 'Blood_Pressure')\n",
    "    df = log_norm(df, 'Creatinine')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51b7cfe-dccf-45a5-be14-7273b6d0c0df",
   "metadata": {},
   "source": [
    "Applying the log normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eb6227-4afa-4786-89fe-1d5e40eec491",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = log_norm_all(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb96bfb-9b3e-4921-b314-8e3ea6703b1d",
   "metadata": {},
   "source": [
    "Checking the variance post application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244c203d-7a12-4ff9-9624-c99b871952f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5d5c70-c131-4b5b-b8d1-fafc65aaa382",
   "metadata": {},
   "source": [
    "Visualizing the change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ffd39e-636e-40d6-8210-c7cd1c5ef3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_log_df = df[['Platelets','log_Platelets',\n",
    "                     'Creatine_Phosphokinase','log_Creatine_Phosphokinase',\n",
    "                     'Blood_Pressure','log_Blood_Pressure',\n",
    "                     'Creatinine','log_Creatinine']\n",
    "                        ]\n",
    "compare_log_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1bd88d-afb1-4e98-8fa9-e4b3e96269e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for columns in compare_log_df.columns:\n",
    "    fig = px.histogram(compare_log_df, x=columns, \n",
    "                       marginal='box',\n",
    "                       title=f'Histogram of {columns}')\n",
    "    fig.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088de396-04eb-4602-92ac-5854334c7c7d",
   "metadata": {},
   "source": [
    "### Correlation between features\n",
    "* Target is positively correlated to Age, Creatinine and BMI\n",
    "* Target is negatively correlated to Sodium  \n",
    "* Age is correlated to Creatinine and BMI and target\n",
    "* Sodium is negatively correlated to target and Creatinine\n",
    "* Sodium and BMI is negatively correlated\n",
    "* Creatinie is highly correlated to target variable\n",
    "* Creatine_Phosphokinase and Hemoglobin are correlated\n",
    "* BMI is positively correlated to Age, Creatinie, Blood_Pressure\n",
    "* BMI is negatively correlated to Hemoglobin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bf8ff4-5972-4e1e-ac2e-926586f239da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449357dc-cda1-49cf-9a15-1f05be5fdfc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = sns.pairplot(df, hue=\"Survive\", palette=\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d4a6f2-038d-4088-9827-ef26e696ace8",
   "metadata": {},
   "source": [
    "### Comparing categoricial and continuous features\n",
    "* No meaningful differences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bb2f92-7995-4cd1-af14-268d68f1bd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641792dd-6605-4857-b2ae-a2b8724019ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for columns in df.columns:\n",
    "    fig = px.histogram(df, x=columns, \n",
    "                       color='Gender',\n",
    "                       title=f'Histogram of {columns}')\n",
    "    fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddb3733-e269-4a36-a3ed-8c7c5960cae3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for columns in df.columns:\n",
    "    fig = px.histogram(df, x=columns, \n",
    "                       color='Smoke',\n",
    "                       title=f'Histogram of {columns}')\n",
    "    fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc669ade-7f9b-44d9-897f-1bd1491d1efd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for columns in df.columns:\n",
    "    fig = px.histogram(df, x=columns, \n",
    "                       color='Diabetes',\n",
    "                       title=f'Histogram of {columns}')\n",
    "    fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0950e0-1502-4f1b-8dcf-ac15dd6eac21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for columns in df.columns:\n",
    "    fig = px.histogram(df, x=columns, \n",
    "                       color='Ejection_Fraction',\n",
    "                       title=f'Histogram of {columns}')\n",
    "    fig.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461759b5-8552-48aa-8d36-f883f3a89fb5",
   "metadata": {},
   "source": [
    "### Summary of EDA\n",
    "##### Univariate \n",
    "* Imbalanced class for target variable at positive 32% vs negative 68%\n",
    "* Gender Male 65% Female 35%\n",
    "* Non Smoker 67% Smoker 33%\n",
    "* Diabetes 20%, Diabetes 21%, Normal 59% \n",
    "* Sodium, Creatinine, Platelets, Creatine_Phosphokinase needs to handle outliers\n",
    "* Creatinine and BMI have differences in terms of positive and negative classes\n",
    "* Log transformation performed on Multiple columns to reduce the variance \n",
    "\n",
    "##### Bivariate\n",
    "* ~5% of smokers are females\n",
    "* Ejection Fraction has a trend of higher mortality with increased heart strength\n",
    "* BMI and Creatinine has noticeable difference between positive and negative classes\n",
    "\n",
    "##### Manipulations done up to this point\n",
    "* Value Standardization for Survive, Smoke, Ejection_Fraction and Age\n",
    "* Removing observations with missing data from Creatinine\n",
    "* Duplicates dropped based on ID\n",
    "* Features created = BMI\n",
    "* Features dropped = ID, Weight, Height, Favourite Colour\n",
    "* Log normalization for Platelets, Blood_Pressure, Creatinine and Creatine_Phosphokinase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2548a3e-d2a1-4c30-80b8-4926e6b65dc6",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "---\n",
    "##### Handling of outliers\n",
    "* Sodium, Creatinine, Platelets, Creatine_Phosphokinase needs to remove outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd27cb5-5f60-4c1f-b859-305e706b8358",
   "metadata": {},
   "source": [
    "Visualizing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfa6668-f696-4416-93a4-687dbd0d19f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for columns in df.columns:\n",
    "    fig = px.histogram(df, x=columns, \n",
    "                       color='Survive',\n",
    "                       color_discrete_map={0:'red', 1:'blue'},\n",
    "                       marginal='box',\n",
    "                       title=f'Histogram of {columns}', \n",
    "                        )\n",
    "    fig.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0dda47-d3db-4a6b-8a3f-ff12e371ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier(df: pd.DataFrame, target_column:str , lower_limit_percentile, upper_limit_percentile):\n",
    "    '''This function removes outliers from a given feature\n",
    "    Parameters:\n",
    "    df: Pandas Dataframe\n",
    "    target_column: Feature name\n",
    "    lower_limit_percentile: lower limit of values to keep\n",
    "    upper_limit_percentile: upper limit of values to keep\n",
    "    \n",
    "    Returns:\n",
    "    df: Feature with outliers removed'''\n",
    "    lower_lim = df[target_column].quantile(lower_limit_percentile)\n",
    "    upper_lim = df[target_column].quantile(upper_limit_percentile)\n",
    "    df = df[(df[target_column] < upper_lim) & (df[target_column] > lower_lim)]\n",
    "    return df\n",
    "\n",
    "def remove_outlier_all(df: pd.DataFrame):\n",
    "    df = remove_outlier(df, 'Sodium', .05,0.95)\n",
    "    df = remove_outlier(df, 'log_Platelets', .05,0.95)\n",
    "    df = remove_outlier(df, 'log_Creatine_Phosphokinase', .05,0.95)\n",
    "    df = remove_outlier(df, 'log_Creatinine', .05,0.95)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88610621-d3f4-47ae-819e-8a8bfd9e3723",
   "metadata": {},
   "source": [
    "* Checking dataframe after removal of outliers\n",
    "* Left with 8209 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401af5bd-ef54-49c3-8775-cd14d1080c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_outlier_all(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b30d6c-1a8b-40a0-ac98-dfd9664b2079",
   "metadata": {},
   "source": [
    "* Visualize dataset after outlier treatment to see effects\n",
    "* log_Creatinine and log_Platelets still has outliers\n",
    "* Will not proceed with a smaller range as the number of observation would become too small. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30b98c2-8076-4634-aa01-59184031e156",
   "metadata": {},
   "source": [
    "##### Encoding Features\n",
    "* Label encoding for Gender, Smoke as the values do not have a hiearchy within them\n",
    "* Ordinal Encoding for Diabetes and Ejection Fraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932027a8-b3fd-49d6-aaa3-0897b1eac4b0",
   "metadata": {},
   "source": [
    "Label coding for Gender and Smoke features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a401860-c81d-4412-a10f-ed500f90d938",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['Gender'] = label_encoder.fit_transform(df['Gender'])\n",
    "df['Smoke'] = label_encoder.fit_transform(df['Smoke'])\n",
    "df['Survive'] = label_encoder.fit_transform(df['Survive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bbe170-b34f-4ec5-965d-9592f4c9a5f2",
   "metadata": {},
   "source": [
    "Checking if encoding was applied correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c17805-b8ba-4167-973c-21b975f797d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Gender'].unique())\n",
    "print(df['Smoke'].unique())\n",
    "print(df['Survive'].unique())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96f3aa2-39c3-4ae7-b24a-5cafa8309260",
   "metadata": {},
   "source": [
    "Ordinal Enconding for Diabetes and Ejection_Fraction feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccf1117-5fa7-4619-b6ba-4e090130bbfc",
   "metadata": {},
   "source": [
    "Getting the unique values of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b16833-beb9-4633-89e6-02d787799fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Diabetes'].unique())\n",
    "print(df['Ejection_Fraction'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d507af3-3150-428b-8097-2290cea8b7ad",
   "metadata": {},
   "source": [
    "Creating the list in ranked order for initializing of OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c704721c-3501-40b2-ae03-0c557b6e8423",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_rank = ['Normal', 'Pre-diabetes', 'Diabetes']\n",
    "ef_rank = ['Low', 'Normal', 'High']\n",
    "diabetes_encoder = OrdinalEncoder(categories = [diabetes_rank])\n",
    "ef_encoder = OrdinalEncoder(categories = [ef_rank])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702d7c52-5f68-4ba4-8a8e-63b0d2449b44",
   "metadata": {},
   "source": [
    "Applying the encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e5053c-61b8-4889-8a42-80578eb0cd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Diabetes'] = diabetes_encoder.fit_transform(df[['Diabetes']])\n",
    "df['Ejection_Fraction'] = ef_encoder.fit_transform(df[['Ejection_Fraction']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e69630-b996-46ee-897f-cb09329adf88",
   "metadata": {},
   "source": [
    "Checking if encoding was applied correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a53102-16e9-4d37-af76-bb3b0d88e32e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df['Diabetes'].unique())\n",
    "print(df['Ejection_Fraction'].unique())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1e38f9-837f-4620-8555-3c98b5a92ddb",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fb7506-98df-4d46-8da4-414bda911050",
   "metadata": {},
   "source": [
    "Checking data type - all correct\n",
    "* Data is mild imbalanced with minority contributing to 30%. \n",
    "* Only stratification of the data will be applied for now\n",
    "* Oversampling or undersampling techniques will be considered if problems arises later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2a5143-76da-4454-8722-bef985c59a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "print(df['Survive'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730b456b-30e1-43f4-8232-fe016f1eaeeb",
   "metadata": {},
   "source": [
    "Defining X and Y \n",
    "* Trying a small feature set to compare results, using only as little features as possible\n",
    "* As the model aims to aid doctors in preemptive medical treatments\n",
    "* The features used should prioritise information that a readily avaluable (e.g bmi, gender, age)\n",
    "* Instead of some information that might require additional testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c6c9de-284e-4d4a-985d-71e8bda1a35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This set has overfitting on all model less logreg (Recall score 0.73)\n",
    "# X = df.drop(columns = ['Survive','Platelets','Creatine_Phosphokinase','Blood_Pressure','Creatinine'])\n",
    "\n",
    "# Adding on interesting features on the light weight features) (Recall score 0.72)\n",
    "# X = df[['Age','log_Creatinine','BMI','Sodium','Gender','Smoke','Ejection_Fraction']]\n",
    "\n",
    "# Trying this set with minimal features (light weight model) (Recall score 0.74)\n",
    "X = df[['Age','log_Creatinine','BMI','Sodium']] \n",
    "y = df['Survive']\n",
    "print(X.shape,y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bab6c79-e545-4c81-91d2-350841087ea4",
   "metadata": {},
   "source": [
    "Checking the dataframe to confirm the features for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1826dc-4435-4b1f-8709-43f3c25d3e44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075406a8-ac57-4792-a9a1-39b377a01f1c",
   "metadata": {},
   "source": [
    "Checking the dataframe to confirm the target feature for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d465aef-12da-4b4c-bd6e-810e47f8d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992fcd1d-5ff3-4eed-9514-88ee02ec2c35",
   "metadata": {},
   "source": [
    "Splitting dataset into 80% temp, 20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbc7768-fc5c-4766-9fce-6b2151548bd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=21)\n",
    "print(X_temp.shape, X_test.shape, y_temp.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6357b3-6a3b-4ec2-9f7f-f59438f5dbf1",
   "metadata": {},
   "source": [
    "Splits the 80% temp dataset into 60% train 20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b19d870-cc3a-405c-8934-3aad590e5d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=21)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d76c9f-d441-4918-af68-2010395ad272",
   "metadata": {},
   "source": [
    "Verifying the number of positive and negative classes after train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbe42e9-69ec-424f-9ba3-1effd9f2ecb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Number of observation in the target variable is')\n",
    "print(y.value_counts())\n",
    "print()\n",
    "print(f'Number of observation for target variable in Training set is')\n",
    "print(y_train.value_counts())\n",
    "print()\n",
    "print(f'Number of observation for target variable in Validation set is')\n",
    "print(y_val.value_counts())\n",
    "print()\n",
    "print(f'Number of observation for target variable in Testing set is')\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e799fd-1c1c-4268-8855-b371e45426ef",
   "metadata": {},
   "source": [
    "* Scaling data from X_train\n",
    "* Applying the scaling information onto X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006e2ff6-3cb4-4653-9356-11943f9451c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6653f452-f4a5-46cb-abb2-67e7e00dea1a",
   "metadata": {},
   "source": [
    "Veriyfing the changes applied to both training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa648f7-692c-4196-8c94-2df3a6f292c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "X_train_scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1475770f-64b1-4146-970b-d049e712c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_scaled_df.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dfb3f0-2941-4f30-98ce-7b8146dfdafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
    "X_test_scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26deeea6-4be2-446e-9b7b-6027732e2ddf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X_test_scaled_df.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60b2445-0870-4a4c-8e17-cc4ceb3970b4",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a5a049-9394-4bb0-ac10-2ddcddb4a6a6",
   "metadata": {},
   "source": [
    "### Choosing the correct metrics to evaluate models \n",
    "* Precision measures ratio of true positives against **all positive predictions** were correct\n",
    "* Recall measures ratio of true positives against **all positive truths** in dataset\n",
    "* False Positive = ( Model predicts 1, Truth is 0) (model says patient would survive when they **wont**)\n",
    "* False Negative = ( Model predicts 0,Truth is 1) (model says patient would not survive when they **will**)\n",
    "* In the context of the problem, it is more important to know the survival of the patient to administer preemptive treatment.\n",
    "* A false positive is more costly than a false negative\n",
    "* Therefore, it is more important to identify patients who will survive correctly. (Low False Positives rate)\n",
    "* In ML context, this means to observe a **high recall** rate (instead of precision). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f016692-0af9-4dd5-bc07-38660c70cde8",
   "metadata": {},
   "source": [
    "## Training a Logistic Regression base model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc2fc3a-039c-495f-bff9-e981ba581d6c",
   "metadata": {},
   "source": [
    "* Training a base model to observe performance\n",
    "* Accuracy score of 0.852 on validation data\n",
    "* Accuracy score of 0.857 on training data\n",
    "* As the scores are similar, it can be concluded that this model is not overfitting or under fitting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cef6ce-2489-4d2f-ada8-994d298f49d5",
   "metadata": {},
   "source": [
    "Generating a base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450dc743-3935-4c86-b9e6-ae2fe51c0358",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate Model\n",
    "logreg_model = LogisticRegression()\n",
    "\n",
    "# Fit the model with training data\n",
    "logreg = logreg_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Passing in scaled validation feature data (X_val_scaled) in fitted model to obtain predictions \n",
    "logreg_y_predict = logreg.predict(X_val_scaled)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_val, logreg_y_predict).ravel()\n",
    "lr_validation_set_score = accuracy_score(y_val, logreg_y_predict)\n",
    "# Printing the results when comparing predicts of scaled validation feature data (X_val_scaled) against ground truth (y_val)\n",
    "print(confusion_matrix(y_val, logreg_y_predict))\n",
    "print(tn, fp, fn, tp)\n",
    "print(classification_report(y_val, logreg_y_predict))\n",
    "print(accuracy_score(y_val, logreg_y_predict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e965ad-c7ae-429c-9c72-fb67e3b09a96",
   "metadata": {},
   "source": [
    "* The recall score is abit low. \n",
    "* We can use accuracy score of both training and test set to determine if model is underfitting or overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00da4e00-84f4-474f-a6fd-ffea50bbf669",
   "metadata": {},
   "source": [
    "Generating predictions on the training data using the model (trained with training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8490965c-32de-4e92-b68d-cdd522e236cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block of code uses the same fitted model above generate predictions on the training data itself \n",
    "# if the score is higher than the validation (testing) set, then it is an overfitting model  \n",
    "lr_train_y_predict = logreg_model.predict(X_train_scaled)\n",
    "lr_training_set_score = accuracy_score(y_train, lr_train_y_predict)\n",
    "print(confusion_matrix(y_train, lr_train_y_predict))\n",
    "print(tn, fp, fn, tp)\n",
    "print(classification_report(y_train, lr_train_y_predict))\n",
    "print(accuracy_score(y_train, lr_train_y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc7bfb4-2223-4844-b5b7-e5f351518362",
   "metadata": {},
   "source": [
    "Training accuracy is slightly higher than validation accuracy by a small amount\n",
    "Thus the model is generalising well to underseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8884f872-d88d-4ce5-a2cc-ad98a204ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The accuracy on the training set is '+ str(lr_training_set_score))\n",
    "print(f'The accuracy on the validation set is '+ str (lr_validation_set_score))\n",
    "\n",
    "if lr_training_set_score > lr_validation_set_score:\n",
    "    print (f'This model might be overfitting')\n",
    "else:\n",
    "    print (f'This model might be underfitting')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b77efc8-976c-4400-bb95-67731287c091",
   "metadata": {},
   "source": [
    "* Performing cross_validation on the trained logistic regression model\n",
    "* Using the model trained earlier, scaled training data, 10 folds and looking at recall\n",
    "* Average recall score was 0.74\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677ff946-82f9-4601-841a-f7433cfc2a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the scoring matrix \n",
    "recall_scorer = make_scorer(recall_score)\n",
    "# Kfold\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state = 21)\n",
    "# Obtaining the cross_validation_scores (recall)\n",
    "cv_results = cross_val_score(logreg_model, X_train_scaled, y_train, cv=kf, scoring=recall_scorer)\n",
    "\n",
    "print(cv_results)\n",
    "print(cv_results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3bcd2a-49c7-4421-b3c4-7b7f2d16db3e",
   "metadata": {},
   "source": [
    "* Accuracy score give a general gauge of how well the model is performing\n",
    "* If training accuracy is high but testing accuracy is low, means the model is overfitting \n",
    "* If both training and testing accuracy is low this could be model underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a47ad1e-b4a0-4cf9-a5ce-70653c58ceb6",
   "metadata": {},
   "source": [
    "## Generate a base model using all classifiers\n",
    "---\n",
    "* Based on recall score, the weaker models are Logistic_Regression are SVM. \n",
    "* As most models have comparable results . It would make sense to choose models that are more efficient and easier to train\n",
    "* This will reduce training times and require less computation resources\n",
    "* Models chosen in the end KNN, Decision Tree and XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77d2c0c-b2e1-40c9-a22a-f2ff8527eda7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = {\"Logistic Regression\": LogisticRegression(), \n",
    "          \"KNN\": KNeighborsClassifier(),\n",
    "          \"Random_Forest\": RandomForestClassifier(),\n",
    "          \"Decision_Tree\": DecisionTreeClassifier(),\n",
    "          \"SVM\": svm.SVC(),\n",
    "          \"XGB\": xgb.XGBClassifier(),        \n",
    "         }\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    model_y_predict = model.predict(X_val_scaled)\n",
    "    print(confusion_matrix(y_val, model_y_predict))\n",
    "    print(classification_report(y_val, model_y_predict))\n",
    "    print(f'The recall score is of '+ f'{name}: {recall_score(y_val, model_y_predict)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70832f4f-7cf7-4f00-99e5-e847def1b547",
   "metadata": {},
   "source": [
    "\n",
    "* performing kfold cross validation to get a more accurate metric on the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fa9f47-330b-4546-b43f-1b533a54bf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for model in models.values():\n",
    "    kf = KFold(n_splits=10, random_state=21, shuffle=True)\n",
    "    cv_results = cross_val_score(model, X_train_scaled, y_train, cv=kf, scoring=recall_scorer)\n",
    "    results.append(cv_results)\n",
    "plt.boxplot(results, labels=models.keys())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a0eed7-0cd0-4113-853e-96fe2a7a2ff7",
   "metadata": {},
   "source": [
    "# Checking of fitting for KNN, Decision Tree and XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1af594-51c3-4fe8-99e5-843cbec3ff70",
   "metadata": {},
   "source": [
    "KNN cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219a65c8-2e6c-45a8-aa83-4dc26da103b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate Model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Fit the model with training data\n",
    "knn = knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Passing in scaled validation feature data (X_val_scaled) in fitted model to obtain predictions \n",
    "knn_y_predict = knn.predict(X_val_scaled)\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_val, knn_y_predict).ravel()\n",
    "knn_validation_set_score = accuracy_score(y_val, knn_y_predict)\n",
    "# Printing the results when comparing predicts of scaled validation feature data (X_val_scaled) against ground truth (y_val)\n",
    "print(confusion_matrix(y_val, knn_y_predict))\n",
    "print(tn, fp, fn, tp)\n",
    "print(classification_report(y_val, knn_y_predict))\n",
    "print(accuracy_score(y_val, knn_y_predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f6f03b-36e6-41c8-bbb1-246b73a7721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block of code uses the same fitted model above generate predictions on the training data itself \n",
    "# if the score is higher than the validation (testing) set, then it is an overfitting model  \n",
    "knn_train_y_predict = knn.predict(X_train_scaled)\n",
    "knn_training_set_score = accuracy_score(y_train, knn_train_y_predict)\n",
    "print(confusion_matrix(y_train, knn_train_y_predict))\n",
    "print(tn, fp, fn, tp)\n",
    "print(classification_report(y_train, knn_train_y_predict))\n",
    "print(accuracy_score(y_train, knn_train_y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04c65c0-ee91-4f1f-a5c7-92c6a4f2618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The accuracy on the training set is '+ str(knn_training_set_score))\n",
    "print(f'The accuracy on the validation set is '+ str (knn_validation_set_score))\n",
    "\n",
    "if knn_training_set_score > knn_validation_set_score:\n",
    "    print (f'This model might be overfitting')\n",
    "else:\n",
    "    print (f'This model might be underfitting')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39485ec9-630f-4299-84e0-28788c5266f1",
   "metadata": {},
   "source": [
    "* Performing cross_validation on the trained logistic regression model\n",
    "* Using the model trained earlier, scaled training data, 10 folds and looking at recall\n",
    "* Lowest recall score was 0.97\n",
    "* Highest recall score was 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9a1df3-8ece-44eb-abee-663e5a7205fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the scoring matrix \n",
    "recall_scorer = make_scorer(recall_score)\n",
    "\n",
    "# Kfold\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state = 21)\n",
    "# Obtaining the cross_validation_scores (recall)\n",
    "knn_cv_results = cross_val_score(knn, X_train_scaled, y_train, cv=kf, scoring=recall_scorer)\n",
    "\n",
    "print(knn_cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc03ba66-4587-4639-8b79-95ab171d7b09",
   "metadata": {},
   "source": [
    "Decision Tree cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d019344-85f2-448c-8456-e14f8b7adbb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate Model\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "# Fit the model with training data\n",
    "dt = dt.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "# Passing in scaled validation feature data (X_val_scaled) in fitted model to obtain predictions \n",
    "dt_y_predict = dt.predict(X_val_scaled)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_val, dt_y_predict).ravel()\n",
    "dt_validation_set_score = accuracy_score(y_val, dt_y_predict)\n",
    "# Printing the results when comparing predicts of scaled validation feature data (X_val_scaled) against ground truth (y_val)\n",
    "print(confusion_matrix(y_val, dt_y_predict))\n",
    "print(tn, fp, fn, tp)\n",
    "print(classification_report(y_val, dt_y_predict))\n",
    "print(accuracy_score(y_val, dt_y_predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a04ff5-5978-40d0-bae6-db4f51176d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block of code uses the same fitted model above generate predictions on the training data itself \n",
    "# if the score is higher than the validation (testing) set, then it is an overfitting model  \n",
    "dt_train_y_predict = dt.predict(X_train_scaled)\n",
    "dt_training_set_score = accuracy_score(y_train, dt_train_y_predict)\n",
    "print(confusion_matrix(y_train, dt_train_y_predict))\n",
    "print(tn, fp, fn, tp)\n",
    "print(classification_report(y_train, dt_train_y_predict))\n",
    "print(accuracy_score(y_train, dt_train_y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5d56d9-3839-4bed-b55b-52b6e9390aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The accuracy on the training set is '+ str(dt_training_set_score))\n",
    "print(f'The accuracy on the validation set is '+ str (dt_validation_set_score))\n",
    "\n",
    "if dt_training_set_score > dt_validation_set_score:\n",
    "    print (f'This model might be overfitting')\n",
    "else:\n",
    "    print (f'This model might be underfitting')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850c8524-0610-4e45-b4c2-a6e500a53f78",
   "metadata": {},
   "source": [
    "* Performing cross_validation on the trained logistic regression model\n",
    "* Using the model trained earlier, scaled training data, 10 folds and looking at recall\n",
    "* Lowest recall score was 0.99\n",
    "* Highest recall score was 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d628750e-a1c5-4e01-a62a-e896d268c82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the scoring matrix \n",
    "recall_scorer = make_scorer(recall_score)\n",
    "\n",
    "# Kfold\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state = 21)\n",
    "\n",
    "# Obtaining the cross_validation_scores (recall)\n",
    "dt_cv_results = cross_val_score(dt, X_train_scaled, y_train, cv=kf, scoring=recall_scorer)\n",
    "\n",
    "print(dt_cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa6afcf-8fe9-4b96-a86f-a6fd01d68eab",
   "metadata": {},
   "source": [
    "XGB cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455ceab4-3acf-49b4-a8ec-8e071dbf67bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate Model\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "# Fit the model with training data\n",
    "xgb_model = xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Passing in scaled validation feature data (X_val_scaled) in fitted model to obtain predictions \n",
    "xgb_y_predict = xgb_model.predict(X_val_scaled)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_val, xgb_y_predict).ravel()\n",
    "xgb_validation_set_score = accuracy_score(y_val, xgb_y_predict)\n",
    "# Printing the results when comparing predicts of scaled validation feature data (X_val_scaled) against ground truth (y_val)\n",
    "print(confusion_matrix(y_val, xgb_y_predict))\n",
    "print(tn, fp, fn, tp)\n",
    "print(classification_report(y_val, xgb_y_predict))\n",
    "print(accuracy_score(y_val, xgb_y_predict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c47807-9683-407c-9cb9-e3c0fc9e4316",
   "metadata": {},
   "source": [
    "* We can use accuracy score of both training and test set to determine if model is underfitting or overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9242c410-7d21-4e01-acf6-568c34ddc8f4",
   "metadata": {},
   "source": [
    "Generating predictions on the training data using the model (trained with training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27d1229-7c7c-45f5-a6d6-b752af478726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block of code uses the same fitted model above generate predictions on the training data itself \n",
    "# if the score is higher than the validation (testing) set, then it is an overfitting model  \n",
    "xgb_train_y_predict = xgb_model.predict(X_train_scaled)\n",
    "xgb_training_set_score = accuracy_score(y_train, xgb_train_y_predict)\n",
    "print(confusion_matrix(y_train, xgb_train_y_predict))\n",
    "print(tn, fp, fn, tp)\n",
    "print(classification_report(y_train, xgb_train_y_predict))\n",
    "print(accuracy_score(y_train, xgb_train_y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1e591e-c10a-48c4-b832-2834dd397383",
   "metadata": {},
   "source": [
    "Training accuracy is slightly higher than validation accuracy by a small amount\n",
    "Thus the model is generalising well to underseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7adf52-50fa-4b17-8b4b-ee33e0497a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The accuracy on the training set is '+ str(xgb_training_set_score))\n",
    "print(f'The accuracy on the validation set is '+ str (xgb_validation_set_score))\n",
    "\n",
    "if xgb_training_set_score > xgb_validation_set_score:\n",
    "    print (f'This model might be overfitting')\n",
    "else:\n",
    "    print (f'This model might be underfitting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6a6a64-8db0-4446-8bad-e94044aaae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the scoring matrix \n",
    "recall_scorer = make_scorer(recall_score)\n",
    "\n",
    "# Kfold\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state = 21)\n",
    "\n",
    "# Obtaining the cross_validation_scores (recall)\n",
    "cv_results = cross_val_score(xgb_model, X_train_scaled, y_train, cv=kf, scoring=recall_scorer)\n",
    "\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b192858-6fe0-4411-ac91-c977f0a3ac32",
   "metadata": {},
   "source": [
    "## Tuning hyperparameters for chosen models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edeb818-2e55-4d05-928a-c7813fb4765f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "* Models are showing good performance, however with some parameter tuning, the performance might be able to perform better\n",
    "* For decision tree only as KNN and XGBoost are achieving good performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d476981-1c40-4864-8dbe-3778ff0706c1",
   "metadata": {},
   "source": [
    "Randomsearch for KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c0e83d-44c5-4681-a6a6-f843a49b4f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the parameters for tuning \n",
    "param_dist = {\"n_neighbors\": range(2,15),\n",
    "                'weights' : ['uniform','distance'],\n",
    "                'metric' : ['minkowski','euclidean','manhattan']}\n",
    "\n",
    "# Using Randomized Search to reduce the amount of runs to find better hyperparameters.\n",
    "random_search_knn = RandomizedSearchCV(\n",
    "                    estimator = knn,\n",
    "                    param_distributions = param_dist,\n",
    "                    n_iter = 50,\n",
    "                    cv=kf,\n",
    "                    scoring = recall_scorer)\n",
    "\n",
    "random_search_knn.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3d673a-3cfd-4a80-8966-8f9e6268ea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observing scores\n",
    "print(random_search_knn.best_score_)\n",
    "\n",
    "# Best Parameters\n",
    "print(random_search_knn.best_params_)\n",
    "\n",
    "# Best estimator\n",
    "print(random_search_knn.estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d72f3e-bf6a-4c81-98b6-d77cdae158a5",
   "metadata": {},
   "source": [
    "Random search for Decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe92224-f599-4b1a-bf7c-ecc477cc7017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the parameters for tuning\n",
    "param_dist = {\"max_depth\": range(2,15),\n",
    "                \"max_features\": range(2, 15),\n",
    "                \"min_samples_split\": range(2, 15)}\n",
    "\n",
    "# Using Randomized Search to reduce the amount of runs to find better hyperparameters.\n",
    "random_search_dt = RandomizedSearchCV(\n",
    "                    estimator = dt,\n",
    "                    param_distributions = param_dist,\n",
    "                    n_iter = 50,\n",
    "                    cv=kf,\n",
    "                    scoring = recall_scorer)\n",
    "\n",
    "random_search_dt.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc7d774-947f-43a2-b157-3dce84788f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observing scores\n",
    "print(random_search_dt.best_score_)\n",
    "\n",
    "# Best Parameters\n",
    "print(random_search_dt.best_params_)\n",
    "\n",
    "# Best estimator\n",
    "print(random_search_dt.estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7316420b-3763-449f-884b-aabe778cc4d8",
   "metadata": {},
   "source": [
    "Random search XGB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e549a4-f6be-48bc-abc9-4d72cec01c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the parameters for tuning\n",
    "param_dist = {\"eta\": [0.01,0.1,0.2,0.5,1],\n",
    "                \"max_depth\": range(2, 15),\n",
    "                }\n",
    "\n",
    "# Using Randomized Search to reduce the amount of runs to find better hyperparameters.\n",
    "random_search_xgb = RandomizedSearchCV(\n",
    "                    estimator = xgb_model,\n",
    "                    param_distributions = param_dist,\n",
    "                    n_iter = 50,\n",
    "                    cv=kf,\n",
    "                    scoring = recall_scorer)\n",
    "\n",
    "random_search_xgb.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed07e09b-e139-43b4-aaeb-c25e7b9ee876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observing scores\n",
    "print(random_search_xgb.best_score_)\n",
    "\n",
    "# Best Parameters\n",
    "print(random_search_xgb.best_params_)\n",
    "\n",
    "# Best estimator\n",
    "print(random_search_xgb.estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49a0ca1-95f4-4d41-bebe-3f403ed8990c",
   "metadata": {},
   "source": [
    "* Obtained best parameters for both,\n",
    "* Testing models with new parameters on validation set again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9923cb8-0dd6-4096-b762-2ef171143d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Model\n",
    "dt_best_params = DecisionTreeClassifier(min_samples_split = 7, max_features= 5, max_depth= 12)\n",
    "\n",
    "\n",
    "# Fit the model with training data\n",
    "dt_best_params = dt_best_params.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "# Passing in scaled validation feature data (X_val_scaled) in fitted model to obtain predictions \n",
    "dt_best_y_predict = dt_best_params.predict(X_val_scaled)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_val, dt_best_y_predict).ravel()\n",
    "dt_best_validation_set_score = accuracy_score(y_val, dt_best_y_predict)\n",
    "# Printing the results when comparing predicts of scaled validation feature data (X_val_scaled) against ground truth (y_val)\n",
    "print(confusion_matrix(y_val, dt_best_y_predict))\n",
    "print(tn, fp, fn, tp)\n",
    "print(classification_report(y_val, dt_best_y_predict))\n",
    "print(accuracy_score(y_val, dt_best_y_predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4ab5d8-14f6-430c-8c79-389ce92bf351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block of code uses the same fitted model above generate predictions on the training data itself \n",
    "# if the score is higher than the validation (testing) set, then it is an overfitting model  \n",
    "dt_best_train_y_predict = dt.predict(X_train_scaled)\n",
    "dt_best_training_set_score = accuracy_score(y_train, dt_train_y_predict)\n",
    "print(confusion_matrix(y_train, dt_train_y_predict))\n",
    "print(tn, fp, fn, tp)\n",
    "print(classification_report(y_train, dt_train_y_predict))\n",
    "print(accuracy_score(y_train, dt_train_y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6316a935-6d6e-4d99-801b-74446c8adba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The accuracy on the training set is '+ str(dt_best_training_set_score))\n",
    "print(f'The accuracy on the validation set is '+ str (dt_best_validation_set_score))\n",
    "\n",
    "if dt_best_training_set_score > dt_best_validation_set_score:\n",
    "    print (f'This model might be overfitting')\n",
    "else:\n",
    "    print (f'This model might be underfitting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d8d22f-8e50-44d2-bf6b-2c5eb3af64da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the scoring matrix \n",
    "recall_scorer = make_scorer(recall_score)\n",
    "\n",
    "# Kfold\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state = 21)\n",
    "\n",
    "# Obtaining the cross_validation_scores (recall)\n",
    "dt_cv_results = cross_val_score(dt_best_params, X_train_scaled, y_train, cv=kf, scoring=recall_scorer)\n",
    "\n",
    "print(dt_cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c28973-6384-4552-a38f-150b9dabb92a",
   "metadata": {},
   "source": [
    "### After best params are defined, testing the final 3 models on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecafb6df-f6b3-4575-8683-33f0063f630e",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6efb85-69f5-45fa-980e-a7357e26c5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Model\n",
    "dt_final = DecisionTreeClassifier(min_samples_split = 7, max_features= 5, max_depth= 12)\n",
    "\n",
    "\n",
    "# Fit the model with training data\n",
    "dt_final = dt_final.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "# Passing in scaled test feature data (X_test_scaled) in fitted model to obtain predictions \n",
    "dt_final_y_predict = dt_final.predict(X_test_scaled)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, dt_final_y_predict).ravel()\n",
    "dt_best_validation_set_score = accuracy_score(y_test, dt_final_y_predict)\n",
    "# Printing the results when comparing predicts of scaled validation feature data (X_val_scaled) against ground truth (y_val)\n",
    "print(confusion_matrix(y_test, dt_final_y_predict))\n",
    "print(tn, fp, fn, tp)\n",
    "print(classification_report(y_test, dt_final_y_predict))\n",
    "print(recall_score(y_test, dt_final_y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc33d2c-3314-468f-b483-128b9df5e075",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad5ab93-1c50-4b36-99b0-5fdb9995f27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Model\n",
    "knn_final = KNeighborsClassifier(weights= 'distance', n_neighbors= 10, metric= 'manhattan')\n",
    "\n",
    "# Fit the model with training data\n",
    "knn_final = knn_final.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Passing in scaled test feature data (X_test_scaled) in fitted model to obtain predictions \n",
    "knn_final_y_predict = knn_final.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, knn_final_y_predict).ravel()\n",
    "knn_final_validation_set_score = accuracy_score(y_test, knn_final_y_predict)\n",
    "# Printing the results when comparing predicts of scaled validation feature data (X_val_scaled) against ground truth (y_val)\n",
    "print(confusion_matrix(y_test, knn_final_y_predict))\n",
    "print(tn, fp, fn, tp)\n",
    "print(classification_report(y_test, knn_final_y_predict))\n",
    "print(recall_score(y_test, knn_final_y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da7ea38-af9e-489f-be89-7e60dc285a5a",
   "metadata": {},
   "source": [
    "### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4329ec50-fda7-4ce6-b914-25ccca6c9e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Model\n",
    "xgb_model_final = xgb.XGBClassifier(max_depth = 3, eta = 0.5)\n",
    "\n",
    "# Fit the model with training data\n",
    "xgb_model_final = xgb_model_final.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Passing in scaled test feature data (X_test_scaled) in fitted model to obtain predictions \n",
    "xgb_final_y_predict = xgb_model_final.predict(X_test_scaled)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, xgb_final_y_predict).ravel()\n",
    "xgb_final_validation_set_score = accuracy_score(y_test, xgb_final_y_predict)\n",
    "# Printing the results when comparing predicts of scaled validation feature data (X_val_scaled) against ground truth (y_val)\n",
    "print(confusion_matrix(y_test, xgb_final_y_predict))\n",
    "print(tn, fp, fn, tp)\n",
    "print(classification_report(y_test, xgb_final_y_predict))\n",
    "print(recall_score(y_test, xgb_final_y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b04382-5612-4b19-8db8-443f3b9e24ba",
   "metadata": {},
   "source": [
    "### Exporting models into .pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20142ad-312a-4700-b48c-a128e379514a",
   "metadata": {},
   "source": [
    "* Saves current model as a checkpoint, can be used for deployment if model is deem serviceable\n",
    "* Further tuning for features can be performed and compared to this\n",
    "* Exporting .pkl file allows subsequent deployment endeavours\n",
    "* Such as containerization with dockers\n",
    "* .pkl file can used to generate a prediction when presented with feature data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcf4fe4-80e1-4a42-9ad4-6ddafbe0da72",
   "metadata": {},
   "source": [
    "Creating the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ce766f-43bd-4082-967c-f624ba25a0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"../bg10-Chan_Guan_Ling-162D/model\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a6125e-fb91-46cc-ba35-ed18dbd2f1d0",
   "metadata": {},
   "source": [
    "Exporting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d27b72-2810-4229-a790-621d1adccf2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "joblib.dump(xgb_model_final, '../bg10-Chan_Guan_Ling-162D/model/xgb_model.pkl')\n",
    "joblib.dump(knn_final, '../bg10-Chan_Guan_Ling-162D/model/knn_model.pkl')\n",
    "joblib.dump(dt_final, '../bg10-Chan_Guan_Ling-162D/model/dt_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
